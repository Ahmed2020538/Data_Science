{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data defination ::\n",
    "*********************************\n",
    "* Web scraping is a term for various methods used to collect information from across the Internet. Generally, this is done with software that simulates human Web surfing to collect specified bits of information from different websites. Those who use web scraping programs may be looking to collect certain data to sell to other users, or to to use for promotional purposes on a website.\n",
    "\n",
    "* Web scraping is also called Web data extraction, screen scraping or Web harvesting\n",
    "\n",
    "\n",
    "# Steps of Scraping Sites ::\n",
    "***********************************\n",
    "* Step 1: Find the URL that you want to scrape. For this example, we are going scrape Flipkart website to extract the Price, Name, and Rating of Laptops. ...\n",
    "* Step 2: Find the html source of the Site which we nedd to scrap data from it.\n",
    "* Step 3: Find the data you want to extract. ...\n",
    "* Step 4: Write the code. ...\n",
    "* Step 5: Run the code and extract the data. ...\n",
    "* Step 6: Store the data in a required format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberares\n",
    "\n",
    "import requests as r\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as B\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Function To Scrapping data from website and create CSV_file To Save data on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Weather_data_set(url) :\n",
    "    # Create CSV_File\n",
    "    with open(\"Weather_data_set.csv\" , \"w+\") as f :\n",
    "        writer = csv.DictWriter( f , fieldnames=[\"Day\" , \"Date\" , \"Max_Temp\" , \"Min_Temp\" ,\"Predtion\", \"Wind_pop_info\" , \"Wind_value\"])\n",
    "        writer.writeheader()\n",
    "        # get response from request\n",
    "        res = r.get(url)\n",
    "        # Using Beautifulsoup To make Python understanding html_code\n",
    "        soup = B(res.text , \"html.parser\")\n",
    "        # Found div Which include all adata we need ToScrapping it from website\n",
    "        data = soup.find_all(\"div\" , attrs = {\"class\" : \"day\"})\n",
    "        #Looping To get the data \n",
    "        for d in data :\n",
    "            day = d.find(\"div\" , attrs = {\"class\" : \"title\"}).find(\"b\").get_text()\n",
    "            date = d.find(\"div\" , attrs = {\"class\" : \"title\"}).find(\"span\").get_text()\n",
    "            maxi_temp = d.find(\"div\" , attrs = {\"class\" : \"temps\"}).find(\"b\").get_text().split(\":\")[1].split(\"°\")[0]\n",
    "            mini_temp = d.find(\"div\" , attrs = {\"class\" : \"temps\"}).find(\"span\").get_text().split(\":\")[1].split(\"°\")[0]\n",
    "            win_info = d.find(\"div\" , attrs = {\"class\" : \"wind-popinfo\"}).get_text().split(\"°\")[0]\n",
    "            win_value = d.find(\"div\" , attrs = {\"class\" : \"wind\"}).get_text().split(\"°\")[1].split(\" \")[0]\n",
    "            pred = d.find(\"div\" , attrs = {\"class\" : \"info\"}).find(\"span\").get_text()\n",
    "            # Insert data in The SCV_File\n",
    "            writer.writerow({\"Day\" : day , \"Date\" : date , \"Max_Temp\" :maxi_temp , \"Min_Temp\" : mini_temp , \"Predtion\" : pred , \"Wind_pop_info\" : win_info , \"Wind_value\" :win_value })\n",
    "            #Print data Which we Scrapping it\n",
    "            print(f\"The Day :: {day}\\nThe Date  :: {date}\\nThe MaX_Temp  :: {maxi_temp}\\nThe Min_Temp  ::{mini_temp}\\nThe Predtion  :: {pred}\\nThe Wind_Info :: {win_info}\\\n",
    "        \\nThe Win_Value :: {win_value}\\n\\n========\\n\\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Present data which we scraping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-266d6be201a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mWeather_data_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://eg.freemeteo.com/weather/cairo/7-days/list/?gid=360630&language=english&country=egypt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-a62a7b118d24>\u001b[0m in \u001b[0;36mWeather_data_set\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mmaxi_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"temps\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"°\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mmini_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"temps\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"°\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mwin_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"wind-popinfo\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"°\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "Weather_data_set(\"https://eg.freemeteo.com/weather/cairo/7-days/list/?gid=360630&language=english&country=egypt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading data as Csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weather_data = pd.read_csv(\"Weather_data_set.csv\")\n",
    "Weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Function To Scrapping data from website whithout any Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weather_data(url):\n",
    "    with open(\"Weathers.csv\" , \"w\") as f :\n",
    "        writer = csv.DictWriter(f , fieldnames=[\"Time\" , \"Tempreture\" , \"Hummiduty\" , \"Window\" ])\n",
    "        writer.writeheader()\n",
    "        res = r.get(url)\n",
    "        soup = B(res.text , \"html.parser\")\n",
    "        Time = soup.find_all(\"span\" , attrs = {\"class\" : \"time\"})\n",
    "        Tempreture = soup.find_all(\"span\" , attrs = {\"class\" : \"temp\"})\n",
    "        Hummiduty = soup.find_all(\"span\" , attrs = {\"class\" : \"humid\"})\n",
    "        Window = soup.find_all(\"span\" , attrs = {\"class\" : \"winds\"})\n",
    "        for T in Time :\n",
    "            Times = T.get_text()\n",
    "            for Temp in Tempreture:\n",
    "                Tempretures = Temp.get_text()\n",
    "                for Hummy in Hummiduty :\n",
    "                    Hummidutys = Hummy.get_text()\n",
    "                    for wind in Window :\n",
    "                        Windows =wind.get_text()\n",
    "            writer.writerow({\"Time\" :Times , \"Tempreture\" :Tempretures , \"Hummiduty\" : Hummidutys , \"Window\" :Windows })\n",
    "            print(f\"The Time is :: {Times}\\nThe Tempreture is :: {Tempretures}\\nThe Hummiduty is :: {Hummidutys}\\nThe Window is :: {Windows}\\n----------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time is :: Now\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 20:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 23:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 02:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 08:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 11:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 14:00\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n",
      "The Time is :: 03:25\n",
      "The Tempreture is :: 31°C\n",
      "The Hummiduty is :: 20%\n",
      "The Window is :: 28 Km/h\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "Weather_data(\"https://eg.freemeteo.com/weather/cairo/current-weather/location/?gid=360630&language=english&country=egypt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
