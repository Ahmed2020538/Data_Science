{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifierModel Train Score is :  0.6220472440944882\n",
      "SGDClassifierModel Test Score is :  0.6382978723404256\n",
      "SGDClassifierModel loss function is :  <sklearn.linear_model.sgd_fast.SquaredLoss object at 0x000002787B9B9190>\n",
      "SGDClassifierModel No. of iteratios is :  77\n",
      "----------------------------------------------------\n",
      "Predicted Value for SGDClassifierModel is :  [1 1 1 1 1 1 1 1 1 1]\n",
      "Confusion Matrix is : \n",
      " [[  0  68]\n",
      " [  0 120]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is :  120\n",
      "F1 Score is :  0.6382978723404256\n",
      "Precision Recall Score is :  (0.6382978723404256, 0.6382978723404256, 0.6382978723404256, None)\n",
      "Precision Score is :  0.6382978723404256\n",
      "Recall Score is :  0.6382978723404256\n",
      "Thresholds Value is :  [1]\n",
      "Classification Report is :                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        68\n",
      "           1       0.64      1.00      0.78       120\n",
      "\n",
      "    accuracy                           0.64       188\n",
      "   macro avg       0.32      0.50      0.39       188\n",
      "weighted avg       0.41      0.64      0.50       188\n",
      "\n",
      "AUC Value  :  0.5\n",
      "Zero One Loss Value :  68\n",
      "ROCAUC Score :  0.5\n",
      "fpr Value  :  [0. 1.]\n",
      "tpr Value  :  [0. 1.]\n",
      "thresholds Value  :  [2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#----------------------------------------------------\n",
    "\n",
    "#load breast cancer data\n",
    "\n",
    "BreastData = load_breast_cancer()\n",
    "\n",
    "#X Data\n",
    "X = BreastData.data\n",
    "\n",
    "#y Data\n",
    "y = BreastData.target\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Splitting data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Applying SGDClassifier Model \n",
    "\n",
    "'''\n",
    "#sklearn.linear_model.SGDClassifier(loss='hinge’, penalty=’l2’, alpha=0.0001,l1_ratio=0.15, fit_intercept=True,\n",
    "#                                   max_iter=None,tol=None, shuffle=True, verbose=0, epsilon=0.1,n_jobs=None,\n",
    "#                                   random_state=None, learning_rate='optimal’, eta0=0.0, power_t=0.5,\n",
    "#                                   early_stopping=False, validation_fraction=0.1,n_iter_no_change=5,\n",
    "#                                   class_weight=None,warm_start=False, average=False, n_iter=None)\n",
    "'''\n",
    "\n",
    "SGDClassifierModel = SGDClassifier(penalty='l2',loss='squared_loss',learning_rate='optimal',random_state=33)\n",
    "SGDClassifierModel.fit(X_train, y_train)\n",
    "\n",
    "#Calculating Details\n",
    "print('SGDClassifierModel Train Score is : ' , SGDClassifierModel.score(X_train, y_train))\n",
    "print('SGDClassifierModel Test Score is : ' , SGDClassifierModel.score(X_test, y_test))\n",
    "print('SGDClassifierModel loss function is : ' , SGDClassifierModel.loss_function_)\n",
    "print('SGDClassifierModel No. of iteratios is : ' , SGDClassifierModel.n_iter_)\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "#Calculating Prediction\n",
    "y_pred = SGDClassifierModel.predict(X_test)\n",
    "print('Predicted Value for SGDClassifierModel is : ' , y_pred[:10])\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Confusion Matrix\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix is : \\n', CM)\n",
    "\n",
    "# drawing confusion matrix\n",
    "sns.heatmap(CM, center = True)\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Accuracy Score  : ((TP + TN) / float(TP + TN + FP + FN))\n",
    "AccScore = accuracy_score(y_test, y_pred, normalize=False)\n",
    "print('Accuracy Score is : ', AccScore)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating F1 Score  : 2 * (precision * recall) / (precision + recall)\n",
    "# f1_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)\n",
    "\n",
    "F1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "print('F1 Score is : ', F1Score)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Precision recall Score :  \n",
    "#metrics.precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=\n",
    "#                                        None, warn_for = ('precision’,’recall’, ’f-score’), sample_weight=None)\n",
    "\n",
    "PrecisionRecallScore = precision_recall_fscore_support(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "print('Precision Recall Score is : ', PrecisionRecallScore)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Precision Score : (Specificity) #(TP / float(TP + FP))  \n",
    "# precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’,sample_weight=None)\n",
    "\n",
    "PrecisionScore = precision_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "print('Precision Score is : ', PrecisionScore)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Recall Score : (Sensitivity) (TP / float(TP + FN))   1 / 1+2  \n",
    "# recall_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)\n",
    "\n",
    "RecallScore = recall_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "print('Recall Score is : ', RecallScore)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Precision recall Curve :  \n",
    "# precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)\n",
    "\n",
    "PrecisionValue, RecallValue, ThresholdsValue = precision_recall_curve(y_test,y_pred)\n",
    "#print('Precision Value is : ', PrecisionValue)\n",
    "#print('Recall Value is : ', RecallValue)\n",
    "print('Thresholds Value is : ', ThresholdsValue)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating classification Report :  \n",
    "#classification_report(y_true, y_pred, labels=None, target_names=None,sample_weight=None, digits=2, output_dict=False)\n",
    "\n",
    "ClassificationReport = classification_report(y_test,y_pred)\n",
    "print('Classification Report is : ', ClassificationReport )\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Area Under the Curve :  \n",
    "\n",
    "fprValue2, tprValue2, thresholdsValue2 = roc_curve(y_test,y_pred)\n",
    "AUCValue = auc(fprValue2, tprValue2)\n",
    "print('AUC Value  : ', AUCValue)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Zero One Loss:  \n",
    "#zero_one_loss(y_true, y_pred, normalize = True, sample_weight = None)\n",
    "\n",
    "ZeroOneLossValue = zero_one_loss(y_test,y_pred,normalize=False) \n",
    "print('Zero One Loss Value : ', ZeroOneLossValue )\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating ROC AUC Score:  \n",
    "#roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None,max_fpr=None)\n",
    "\n",
    "ROCAUCScore = roc_auc_score(y_test,y_pred, average='micro') #it can be : macro,weighted,samples\n",
    "print('ROCAUC Score : ', ROCAUCScore)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Calculating Receiver Operating Characteristic :  \n",
    "#roc_curve(y_true, y_score, pos_label=None, sample_weight=None,drop_intermediate=True)\n",
    "\n",
    "fprValue, tprValue, thresholdsValue = roc_curve(y_test,y_pred)\n",
    "print('fpr Value  : ', fprValue)\n",
    "print('tpr Value  : ', tprValue)\n",
    "print('thresholds Value  : ', thresholdsValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"G:\\\\Data Field\\\\D..S..Course\\\\data.sc.Asem\\\\Data\\Data\\\\2.2 Logistic Regression\\\\heart.csv\"\n",
    "dataset = pd.read_csv(path)\n",
    "dataset.head(20)\n",
    "\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "X\n",
    "y\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "X_train\n",
    "X_test\n",
    "y_train\n",
    "y_test \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train\n",
    "X_test\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = sgd.predict(X_test)\n",
    "y_pred \n",
    "\n",
    "\n",
    "sgd.n_iter_\n",
    "\n",
    "\n",
    "#probability of all values\n",
    "pr = sgd.predict_proba(X_test)[0:10,:]\n",
    "pr\n",
    "\n",
    "\n",
    "#  Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    " \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import median_absolute_error\n",
    "median_absolute_error(y_test, y_pred)\n",
    "\n",
    " \n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='micro')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
